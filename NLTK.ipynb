{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# libraries importeren\n",
    "import nltk\n",
    "from nltk import CFG, grammar, parse, word_tokenize\n",
    "from nltk.grammar import FeatureGrammar\n",
    "from nltk.parse import RecursiveDescentParser, FeatureEarleyChartParser\n",
    "from nltk.parse.generate import generate\n",
    "\n",
    "# functies uit huiswerk\n",
    "def check_sentence(parser, sentence):\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Checking ife provided sentence matches the grammar:\")\n",
    "    print(sentence)\n",
    "    if isinstance(sentence, str):\n",
    "        sentence = sentence.split()\n",
    "    tree_found = False\n",
    "    results = parser.parse(sentence)\n",
    "    for tree in results:\n",
    "        tree_found = True\n",
    "        print(tree)\n",
    "    if not tree_found:\n",
    "        print(sentence, \"Does not match the provided grammar.\")\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return tree_found\n",
    "\n",
    "Test1 = FeatureGrammar.fromstring(\n",
    "    # Van internet gepakt\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    ")\n",
    "# parse hier een string voor een parse tree en alternatieve zinnen\n",
    "Test1_parser = FeatureEarleyChartParser(Test1)\n",
    "# for t in Test1_parser.parse(\"I returned a book to the library\".split()):\n",
    "#     print(t)\n",
    "#\n",
    "#\n",
    "#     for t in Test1_parser.parse(\"I returned a book to the library\".split()):print(t)\n",
    "\n",
    "for s in generate(Test1, n = 10):\n",
    "   if len(list(Test1_parser.parse(s))) != 0:\n",
    "       print(' '.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Thomas\n",
      "[nltk_data]     Meeusen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Thomas Meeusen\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ\n",
      "['ï', 'white', 'other', 'black', 'sixteen', 'queen', 'numbered', 'top', 'square', 'special', 'same', 'own', 'mandatory', 'second', 'seventh', 'first', 'initial', 'straight', 'vertical', 'empty', 'diagonal', 'combined', 'double', 'important', 'certain', 'following', 'enemy', 'next', 'f1', 'short']\n",
      "NNP\n",
      "['»']\n",
      "NN\n",
      "['¿chess', 'game', 'player', 'beginning', 'king', 'chessboard', 'board', 'field', 'corner', 'order', 'record', 'point', 'view', 'bottom', 'row', 'columns', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'name', 'combination', 'letter', 'column', 'number', 'example', 'square', 'a1', 'move', 'exception', 'castling', 'time', 'opponent', 'piece', 'play', 'rest', 'capturing', 'start', 'arrangement', 'rook', 'knight', 'bishop', 'lady', 'note', 'setup', 'color', 'tower', 'horizontal', 'line', 'belonging', 'runner', 'jump', 'horse', 'step', 'direction', 'jumps', 'b1', 'effect', 'pawn', 'd2', 'd3', 'space', 'type', 'rokade', 'check', 'end', 'kind', 'e1', 'c1', 'd1', 'g1']\n",
      "VBZ\n",
      "['is', 'plays', 'has', 'consists', 'piece', 'contains', 'follows', 'begins', 'ends', 'moves', 'makes', 'passes', 'captures', 'does', 'strikes', 'wants', 'casts']\n",
      "DT\n",
      "['a', 'the', 'each', 'all', 'another', 'that', 'an', 'this', 'any', 'no']\n",
      "VBN\n",
      "['played', 'colored', 'laid', 'been', 'given', 'seen', 'named', 'moved', 'removed', 'left', 'captured', 'described', 'allowed', 'run', 'provided', 'done', 'met', 'attacked']\n",
      "IN\n",
      "['by', 'with', 'at', 'of', 'on', 'that', 'in', 'as', 'from', 'for', 'out', 'over', 'between', 'if', 'whether', 'during', 'after']\n",
      "CD\n",
      "['two', 'one', 'eight', '64', '1', '2', '3', '4', '5', '6', '7', '8']\n",
      "NNS\n",
      "['players', 'pieces', 'plays', 'rooks', 'bishops', 'knights', 'pawns', 'fields', 'rows', 'columns', 'moves', 'names', 'turns', 'rules', 'ladies', 'squares', 'capabilities', 'steps', 'horses', 'conditions', 'crosses', 'spaces']\n",
      "CC\n",
      "['and', 'or']\n",
      ":\n",
      "[':', ';']\n",
      ",\n",
      "[',']\n",
      "VBP\n",
      "['are', 'have', 'take', 'say', 'stand', 'differ', 'go']\n",
      "RB\n",
      "['alternately', 'so', 'then', 'not', 'right', 'also', 'again', 'diagonally', 'outward', 'horizontally', 'vertically', 'over', 'straight', 'ahead', 'yet', 'still', 'forward', 'never', 'long', 'similarly']\n",
      "MD\n",
      "['must', 'can', 'may']\n",
      "VB\n",
      "['be', 'capture', 'jump', 'contain', 'move', 'start', 'c3', 'take', 'd4', 'make', 'perform', 'cast', 'cross', 'rook']\n",
      "RP\n",
      "['out', 'over']\n",
      "EX\n",
      "['there']\n",
      "JJR\n",
      "['lower', 'tower']\n",
      "VBD\n",
      "['left', 'jumped']\n",
      "TO\n",
      "['to']\n",
      "POS\n",
      "[\"'s\"]\n",
      "PRP$\n",
      "['its', 'his', 'their']\n",
      "(\n",
      "['(']\n",
      ")\n",
      "[')']\n",
      "VBG\n",
      "['making', 'starting', 'moving', 'according', 'being', 'containing', 'depending', 'doing', 'king']\n",
      "WDT\n",
      "['that', 'which']\n",
      "WRB\n",
      "['how', 'where', 'when']\n",
      ".\n",
      "['.']\n",
      "WP\n",
      "['who']\n",
      "PRP\n",
      "['it']\n",
      "RBS\n",
      "['most']\n",
      "PDT\n",
      "['both']\n"
     ]
    }
   ],
   "source": [
    "# Gebruik deze code om woorden van de corpus te categorizeren.\n",
    "# Packages\n",
    "import nltk\n",
    "from nltk import CFG, grammar, parse, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# dictionary in het formaat: wordclass: [woord1, woord2, etc]\n",
    "chess_dictionary = dict()\n",
    "\n",
    "# toevoegen van woorden\n",
    "def add_chess_dict(rules, left, right):\n",
    "    # If the key does not already exist, initialize it with a list.\n",
    "    if left not in rules:\n",
    "        rules[left] = []\n",
    "    if right  not in rules[left]:\n",
    "        rules[left].append(right)\n",
    "\n",
    "# open hier de corpus!\n",
    "with open(\"chessrules.txt\", \"r\") as a_file:\n",
    "  # itereer door iedere lijn\n",
    "  for line in a_file:\n",
    "    stripped_line = line.strip()\n",
    "    line = word_tokenize(stripped_line)\n",
    "    tagged_line = nltk.pos_tag(line)\n",
    "    # itereer door de gemaakte pairs\n",
    "    for pairs in tagged_line:\n",
    "        add_chess_dict(chess_dictionary, pairs[1], pairs[0])\n",
    "\n",
    "# Gebruik dit voor de ouput\n",
    "for keys,values in chess_dictionary.items():\n",
    "    print(keys)\n",
    "    print(values)\n",
    "\n",
    "\n",
    "#############################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import CFG, grammar, parse, word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "f = open('chessrules.txt')\n",
    "for line in f:\n",
    "    print(line.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Define the cfg grammar.\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> 'DT' 'NN'\n",
    "VP -> 'VB'\n",
    "VP -> 'VB' 'NN'\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Make your POS sentence into a list of tokens.\n",
    "sentence = \"DT NN VB NN\".split(\" \")\n",
    "\n",
    "# Load the grammar into the ChartParser.\n",
    "cp = nltk.ChartParser(grammar)\n",
    "\n",
    "# Generate and print the nbest_parse from the grammar given the sentence tokens.\n",
    "for tree in cp.nbest_parse(sentence):\n",
    "    print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each other field if both this horses given contain f1 or black\n",
      "prps space example , not both this first spaces c3 !\n",
      "be and cross their .\n",
      "an double enemy setup , it cross over if by prps columns .\n",
      "field being an long mandatory d3 .\n",
      "first outward outward rook both no f in\n",
      "and prps knight name left similarly left prps d3 tower so . ?\n",
      "out an g1 depending out to order or b !\n",
      "that ladies vertical f1 ladies then jumped this rows lower\n",
      "and , by , yet jumped ahead seventh to vertical .\n",
      "between diagonally any beginning containing and depending certain square ladies ?\n",
      "prps other rokade jumped h and d1 by on .\n",
      "another go horizontally of all vertical ? tower rbr\n",
      "rbr , straight cross prps own d ?\n",
      "this outward jump or rook of .\n",
      "and combined rooks all knights jumped who and all fields ?\n",
      "out , special same a color containing 7 at !\n",
      "over rbr on again , no c1 player captured .\n",
      "an vertical knights , the bishops take contain 8 two moves prps type ?\n",
      "and when diagonal pieces named whether for .\n",
      "short c and point over their ? yet attacked perform and any second\n",
      "at each bishops right both the black plays numbered enemy !\n",
      "or rbr another numbered done capture similarly during prps rook e1 !\n",
      "or each view type d4 bishops tower !\n",
      "have king and depending combined pawns .\n",
      "any of , or an initial black h , any combined make !\n",
      "by certain runner does where own rules ?\n",
      "prps f1 chess lower turns prps setup runner not left make combined and important\n",
      "left ahead to prps certain conditions jumped over not mandatory\n",
      "met two outward outward f1 ? or any rows\n",
      "beginning still if a own ! rook this vertically vertical game make all straight\n",
      "long over second combination any outward ? contains where next\n",
      "it at an following wants certain . .\n",
      "and the own left over between .\n",
      "the sixteen , 8 4 rooks does how numbered ?\n",
      "or , who been .\n",
      "rbr prps rest and mandatory black bishops been outward\n",
      "rbr , this pawns jumped forward take out or prps number 64 ?\n",
      "and and prps corner h jumped and of ?\n",
      "and outward jumped right how mandatory .\n",
      "still 5 6 turns consists who ?\n",
      "according for outward outward between to the color ?\n",
      "after f and pawn go can so given left with square !\n",
      "and game yet who stand over\n",
      "whether between at who , prps straight number left !\n",
      "or effect laid enemy rules ?\n",
      "initial outward outward jumped then f1 .\n",
      "rbr outward outward jump and who another knights with there .\n",
      "both any spaces from outward or right contain\n",
      "outward prps rows and this white empty bishop for left when seventh\n"
     ]
    }
   ],
   "source": [
    "from nltk import CFG\n",
    "from nltk.grammar import Nonterminal\n",
    "from nltk.parse import ViterbiParser\n",
    "from nltk.parse.generate import generate\n",
    "from random import choices\n",
    "from random import sample\n",
    "\n",
    "MT = 1\n",
    "N_SEN = 50\n",
    "grammar_path = \"lexical rules.txt\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_phrase(grammar, prod = None):\n",
    "    if not prod:\n",
    "        prod = grammar.start()\n",
    "    if prod in grammar._lhs_index:\n",
    "        # Non-terminals\n",
    "        derivations = grammar._lhs_index[prod]\n",
    "        try:\n",
    "            probabilities = [d.prob() for d in derivations]\n",
    "        except AttributeError:\n",
    "            probabilities = None\n",
    "        derivation = choices(derivations, probabilities)[0]\n",
    "        # print(derivation._rhs)\n",
    "        # input()\n",
    "        for d in derivation._rhs:\n",
    "            yield from generate_phrase(grammar, d)\n",
    "    elif prod in grammar._rhs_index:\n",
    "        # Terminals\n",
    "        yield str(prod)\n",
    "\n",
    "def generate_corpus(grammar, prod = None):\n",
    "    yield list(generate_phrase(grammar, prod))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rules = open(grammar_path).read()\n",
    "    grammar = CFG.fromstring(rules)\n",
    "    # print(grammar)\n",
    "    if MT == 1:\n",
    "        sentences = [next(generate_corpus(grammar, Nonterminal('S'))) for s in range(500)]\n",
    "        sentences = sample([s for s in sentences if 4 < len(s) < 15], N_SEN)\n",
    "        for s in sentences:\n",
    "            print(' '.join(s).lower())\n",
    "        # print(sentences)\n",
    "\n",
    "    if MT == 2:\n",
    "        sentences = sample([s for s in generate(grammar, start=Nonterminal('S'), depth = 8, n = 100)], N_SEN)\n",
    "        for s in sentences:\n",
    "            print(' '.join(s).lower())\n",
    "        # print(sentences)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[', -> ', '-LRB-', '-RRB-', '. -> ', ': -> ', 'ADJP ', 'ADVP ', 'CC ->', 'CD ->', 'DT ->', 'EX ->', 'FRAG ', 'IN ->', 'JJ ->', 'JJR -', 'MD ->', 'NML -', 'NN ->', 'NNS -', 'NP ->', 'PDT -', 'POS -', 'PP ->', 'PRP -', 'PRP$ ', 'PRT -', 'RB ->', 'RBS -', 'ROOT ', 'RP ->', 'S -> ', 'SBAR ', 'SYM -', 'TO ->', 'UCP -', 'VB ->', 'VBD -', 'VBG -', 'VBN -', 'VBP -', 'VBZ -', 'VP ->', 'WDT -', 'WHADV', 'WHNP ', 'WP ->', 'WRB -']\n",
      "{', -> ': 64, '-LRB-': 1, '-RRB-': 1, '. -> ': 2, ': -> ': 17, 'ADJP ': 13, 'ADVP ': 34, 'CC ->': 31, 'CD ->': 35, 'DT ->': 180, 'EX ->': 5, 'FRAG ': 10, 'IN ->': 127, 'JJ ->': 67, 'JJR -': 3, 'MD ->': 19, 'NML -': 2, 'NN ->': 241, 'NNS -': 47, 'NP ->': 357, 'PDT -': 1, 'POS -': 10, 'PP ->': 113, 'PRP -': 5, 'PRP$ ': 10, 'PRT -': 2, 'RB ->': 53, 'RBS -': 1, 'ROOT ': 56, 'RP ->': 2, 'S -> ': 129, 'SBAR ': 38, 'SYM -': 2, 'TO ->': 3, 'UCP -': 1, 'VB ->': 23, 'VBD -': 1, 'VBG -': 13, 'VBN -': 30, 'VBP -': 18, 'VBZ -': 65, 'VP ->': 168, 'WDT -': 8, 'WHADV': 10, 'WHNP ': 9, 'WP ->': 1, 'WRB -': 10}\n"
     ]
    }
   ],
   "source": [
    "# import nltk_main as nl\n",
    "import nltk as nltk\n",
    "from nltk import Tree\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "txt = open('chessrules.txt', 'r')\n",
    "lines = txt.readlines()\n",
    "\n",
    "# sentences = nl.get_sentences(txt)\n",
    "\n",
    "def extract_rules(lines):\n",
    "    nlp = StanfordCoreNLP('http://localhost', port=9000, timeout=30000)\n",
    "    file_in = open('grammar_rules.txt', \"w\")\n",
    "    all_rules = []\n",
    "    all_tags = []\n",
    "    count_tags = {}\n",
    "    for sentence in lines:\n",
    "        parsed = nlp.parse(sentence)\n",
    "        t = Tree.fromstring(parsed)\n",
    "        rules = t.productions()\n",
    "        for rule in rules:\n",
    "            all_rules.append(str(rule))\n",
    "            all_tags.append(str(rule)[:5])\n",
    "    unique_rules = sorted(set(all_rules))\n",
    "    print(sorted(set(all_tags)))\n",
    "    for tag in sorted(set(all_tags)):\n",
    "        count = all_tags.count(tag)\n",
    "        count_tags[tag] = count\n",
    "    print(count_tags)\n",
    "    for rule in unique_rules:\n",
    "        count = all_rules.count(rule)\n",
    "        count_tag = count_tags[rule[:5]]\n",
    "        file_in.write(rule + ' [{}]'.format(count/count_tag) + '\\n')\n",
    "    file_in.close()\n",
    "\n",
    "\n",
    "def write_grammar():\n",
    "    file_in = open('grammar_rules.txt', \"r\")\n",
    "    rules = file_in.read()\n",
    "    rules = sorted(set(rules.split('\\n')))\n",
    "    file_in.close()\n",
    "    file_out = open('Grammar_rules_pcfg.txt', \"w\")\n",
    "    for rule in rules:\n",
    "        file_out.write(rule + '\\n')\n",
    "    file_out.close()\n",
    "\n",
    "\n",
    "extract_rules(lines)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}